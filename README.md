# Action Recognition with Vision Transformer (ViT)

**Coursework Implementation - Video Action Recognition using TimeSFormer**

This project implements video action recognition using Vision Transformer (ViT) adapted for video data through TimeSFormer architecture. The system recognizes 25 different actions from the HMDB dataset.

## ğŸ¯ Coursework Requirements Met

- âœ… **Pre-trained TimeSFormer** from `facebook/timesformer-base-finetuned-k400`
- âœ… **HMDB Dataset** with 1,250 videos (50 per category)
- âœ… **8Ã—224Ã—224 clips** with 1/32 sampling rate
- âœ… **80/20 train/validation split**
- âœ… **TensorBoard integration** for training progress visualization
- âœ… **Top-1 and Top-5 accuracy** calculation
- âœ… **Confusion matrix** visualization and analysis
- âœ… **Top-5 predictions analysis** for sample videos
- âœ… **Hyperparameter tuning** experiments
- ğŸ¯ **Target Performance**: 45% top-1, 75% top-5 accuracy

## ğŸš€ Quick Start

### ğŸ“¦ Installation Options

#### Option 1: Standard Installation
```bash
pip install -r requirements.txt
```

#### Option 2: If Network Issues (Colab/Server)
```bash
# Use minimal requirements
pip install -r requirements_minimal.txt

# Or use robust installer
python install_dependencies.py
```

#### Option 3: Manual Core Installation
```bash
# Essential packages only
pip install torch torchvision transformers tensorboard numpy matplotlib Pillow tqdm PyYAML
```

### ğŸƒâ€â™‚ï¸ Running the Project

#### For Google Colab:
```python
!python train_colab.py
```

#### For Local Development:
```bash
python main.py
```

### ğŸ”§ Troubleshooting Installation

**Network Connection Issues:**
- Use `requirements_minimal.txt` for limited connectivity
- Run `python install_dependencies.py` for robust installation with retries
- Install core packages manually if needed

**Memory Issues:**
- Reduce batch size in `config_colab.yaml`
- Use minimal requirements to save space

**Import Errors:**
- Check Python version (3.8+ required)
- Verify virtual environment activation
- Try installing packages individually

## ğŸ“Š TensorBoard Visualization

Monitor training progress in real-time:
```bash
tensorboard --logdir=logs/tensorboard
```

View metrics:
- Training/Validation Loss
- Top-1 and Top-5 Accuracy
- Learning Rate Schedules
- Model Comparisons

## ğŸ—ï¸ Project Structure

```
Action_Recognition_ViT/
â”œâ”€â”€ models/                    # Model architectures
â”‚   â”œâ”€â”€ model_timesformer.py  # Pre-trained TimeSFormer (ViT for video)
â”‚   â”œâ”€â”€ model_vit_separate.py # Standard ViT adaptation
â”œâ”€â”€ src/                      # Core implementation
â”‚   â”œâ”€â”€ train.py             # Training pipeline with TensorBoard
â”‚   â”œâ”€â”€ dataloader.py        # HMDB dataset loader (1/32 sampling)
â”‚   â”œâ”€â”€ evaluate.py          # Evaluation and analysis
â”‚   â””â”€â”€ utils.py             # Utility functions
â”œâ”€â”€ config.yaml              # Training configuration
â”œâ”€â”€ train_colab.py           # Colab-ready training script
â””â”€â”€ requirements.txt         # Dependencies
```

## ğŸ“ˆ Training Features

- **Multi-Model Support**: TimeSFormer, ViT, CNN-LSTM
- **Experiment Management**: Optimizer, learning rate, batch size tuning
- **Real-time Monitoring**: TensorBoard integration
- **Comprehensive Analysis**: Top-5 predictions, confusion matrices
- **Progress Tracking**: Coursework target monitoring (45%/75%)

## ğŸ“ Results and Output Locations

### ğŸ¯ Where Your Results Are Saved

After training completion, all results are organized in these directories:

```
ğŸ“Š RESULTS DIRECTORY STRUCTURE:
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confusion_matrices/              # ğŸ¨ Confusion matrix visualizations
â”‚   â”‚   â”œâ”€â”€ timesformer_confusion_matrix.png
â”‚   â”‚   â”œâ”€â”€ vit_confusion_matrix.png
â”‚   â”‚   â”œâ”€â”€ cnn_lstm_confusion_matrix.png
â”‚   â”‚   â””â”€â”€ optimizer_adam_confusion_matrix.png
â”‚   â”œâ”€â”€ training_logs/                   # ğŸ“ Detailed training progress
â”‚   â”‚   â”œâ”€â”€ timesformer_training.log
â”‚   â”‚   â”œâ”€â”€ vit_training.log
â”‚   â”‚   â””â”€â”€ cnn_lstm_training.log
â”‚   â”œâ”€â”€ experiment_results.json         # ğŸ“‹ Complete experiment data
â”‚   â”œâ”€â”€ comprehensive_analysis.png      # ğŸ” Multi-model comparison
â”‚   â””â”€â”€ experiment_report.txt           # ğŸ“„ Human-readable summary
â”‚
â”œâ”€â”€ models/                             # ğŸ’¾ Trained model weights
â”‚   â”œâ”€â”€ timesformer_best.pth           # ğŸ† Best TimeSFormer model
â”‚   â”œâ”€â”€ vit_best.pth                   # ğŸ† Best ViT model
â”‚   â””â”€â”€ cnn_lstm_best.pth              # ğŸ† Best CNN-LSTM model
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ tensorboard/                    # ğŸ“ˆ TensorBoard files
â”‚   â”‚   â”œâ”€â”€ timesformer_experiment/     # TimeSFormer training logs
â”‚   â”‚   â”œâ”€â”€ vit_experiment/             # ViT training logs
â”‚   â”‚   â””â”€â”€ cnn_lstm_experiment/        # CNN-LSTM training logs
â”‚   â””â”€â”€ training_logs/                  # ğŸ“Š Text-based logs
â”‚       â”œâ”€â”€ training_progress.log
â”‚       â””â”€â”€ experiment_summary.log
```

### ğŸ” Key Files for Coursework Submission

| **File** | **Purpose** | **Location** |
|----------|-------------|--------------|
| ğŸ¯ **Main Results** | TimeSFormer confusion matrix | `results/confusion_matrices/timesformer_confusion_matrix.png` |
| ğŸ“Š **Model Comparison** | All models side-by-side | `results/comprehensive_analysis.png` |
| ğŸ“ **Performance Summary** | Text-based results | `results/experiment_report.txt` |
| ğŸ“ˆ **Training Progress** | TensorBoard visualization | `logs/tensorboard/timesformer_experiment/` |
| ğŸ’¾ **Best Model** | Trained model weights | `models/timesformer_best.pth` |
| ğŸ“‹ **All Metrics** | JSON data format | `results/experiment_results.json` |

### ğŸ’¡ How to Access Results

**View Training Summary:**
```bash
# Text summary
cat results/experiment_report.txt

# JSON results  
python -c "import json; print(json.dumps(json.load(open('results/experiment_results.json')), indent=2))"
```

**View Confusion Matrices:**
```bash
# Open images (Windows)
start results/confusion_matrices/timesformer_confusion_matrix.png
start results/comprehensive_analysis.png
```

**Launch TensorBoard:**
```bash
tensorboard --logdir logs/tensorboard
# Open: http://localhost:6006
```

**Load Trained Models:**
```python
from models.model_timesformer import TimeSFormerActionRecognition
model, checkpoint = TimeSFormerActionRecognition.load_model('models/timesformer_best.pth')
print(f"Best accuracy: {checkpoint['metrics']['best_val_accuracy']:.2f}%")
```

## ğŸ” Analysis Outputs

1. **Confusion Matrices**: For each model and experiment
2. **Top-5 Analysis**: Sample predictions with confidence scores
3. **Training Curves**: Loss and accuracy progression
4. **Model Comparison**: Performance across different configurations
5. **TensorBoard Logs**: Interactive training visualization

## ğŸ¥ Video Processing

- **Input Format**: 8 frames Ã— 224Ã—224 pixels
- **Sampling Rate**: 1/32 (TimeSFormer specification)
- **Preprocessing**: Frame padding, temporal reversal, flickering
- **Data Augmentation**: Random horizontal flip, color jitter

## ğŸ“‹ Usage Examples

### Run Complete Training:
```python
trainer = ActionRecognitionTrainer("config.yaml")
trainer.run_experiments()
```

### Analyze Top-5 Predictions:
```python
trainer.analyze_top5_predictions(model, val_loader, num_samples=10)
```

### View Results:
- Check `results/` for confusion matrices and analysis
- Open TensorBoard for interactive training visualization
- Review `logs/` for detailed training logs

## ğŸ† Performance Targets

- **Minimum Top-1 Accuracy**: 45%
- **Minimum Top-5 Accuracy**: 75%
- **Model**: Pre-trained TimeSFormer adapted for HMDB
- **Dataset**: 25 action categories, 80/20 split
